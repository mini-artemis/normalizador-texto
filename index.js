const fs = require('fs');
const OpenAI = require('openai');

// Conexi√≥n al servidor local de LM Studio
const openai = new OpenAI({
    baseURL: 'http://localhost:1234/v1',
    apiKey: 'not-needed-for-local'
});

async function chatearConModeloLocal() {
    try {
        const promptUsuario = fs.readFileSync('entrada.txt', 'utf-8');
        console.log(`üí¨ Enviando prompt: "${promptUsuario}"`);

        const chatCompletion = await openai.chat.completions.create({
            messages: [
                { role: 'system', content: 'Eres un asistente √∫til y creativo.' },
                { role: 'user', content: promptUsuario }
            ],
            // üëá Aqu√≠ el cambio importante
            model: 'phi-3-mini-4k-instruct',
            temperature: 0.7,
        });

        const respuesta = chatCompletion.choices[0].message.content;
        console.log('\nü§ñ Respuesta del modelo:\n');
        console.log(respuesta);

        fs.writeFileSync('salida.txt', respuesta);
        console.log('\n‚úÖ Respuesta guardada en salida.txt');
    } catch (error) {
        console.error('‚ùå Error:', error.message);
        if (error.code === 'ECONNREFUSED') {
            console.error('üëâ Verifica que el servidor de LM Studio est√© encendido.');
        }
    }
}

chatearConModeloLocal();
