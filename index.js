const fs = require('fs');
// Importamos el SDK oficial de OpenAI
const OpenAI = require('openai');

// 1. Configuramos el cliente de OpenAI
const openai = new OpenAI({
    // Direcci√≥n del servidor local (LM Studio u otro)
    baseURL: 'http://localhost:1234/v1',
    // No importa la clave, solo debe existir
    apiKey: 'not-needed-for-local'
});

// Funci√≥n principal as√≠ncrona
async function chatearConModeloLocal() {
    try {
        // 2. Leemos el prompt desde el archivo de entrada
        const promptUsuario = fs.readFileSync('entrada.txt', 'utf-8');
        console.log(`üì§ Enviando prompt: "${promptUsuario}"`);

        // 3. Llamada al modelo local
        const chatCompletion = await openai.chat.completions.create({
            messages: [
                {
                    role: 'system',
                    content: 'Eres un asistente √∫til, reflexivo y preciso.'
                },
                {
                    role: 'user',
                    content: promptUsuario
                }
            ],
            // üëá Cambiamos aqu√≠ el modelo
            model: 'deepseek-r1-distill-qwen-7b',
            temperature: 0.7,
        });

        // 4. Extraemos y mostramos la respuesta
        const respuesta = chatCompletion.choices[0].message.content;
        console.log('üí¨ Respuesta del Modelo:');
        console.log(respuesta);

        // 5. Guardamos la respuesta en el archivo de salida
        fs.writeFileSync('salida.txt', respuesta);
        console.log('\n‚úÖ Respuesta guardada en "salida.txt"');
    } catch (error) {
        console.error('‚ùå Ha ocurrido un error:');
        if (error.code === 'ECONNREFUSED') {
            console.error('Error: No se pudo conectar. ¬øIniciaste el servidor en LM Studio?');
        } else {
            console.error(error.message);
        }
    }
}

// Ejecutamos la funci√≥n
chatearConModeloLocal();
